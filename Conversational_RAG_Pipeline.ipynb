{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63fd4bd",
   "metadata": {},
   "source": [
    "# ðŸ¦œðŸ”— Conversational RAG with LangChain, Groq, and ChromaDB\n",
    "\n",
    "### **Project Overview**\n",
    "This notebook demonstrates how to build a **Retrieval-Augmented Generation (RAG)** pipeline. The system allows users to chat with external documents (in this case, a technical blog post) that the LLM was not explicitly trained on.\n",
    "\n",
    "**Key Technologies:**\n",
    "* **LangChain:** For orchestration and chain management.\n",
    "* **Groq (Llama 3):** For ultra-fast LLM inference.\n",
    "* **HuggingFace:** For generating open-source text embeddings.\n",
    "* **ChromaDB:** As the local vector store for semantic search.\n",
    "\n",
    "### **Architecture**\n",
    "1.  **Ingest:** Load data from a URL.\n",
    "2.  **Split:** Chunk the text into manageable pieces.\n",
    "3.  **Embed:** Convert chunks into vectors using `all-MiniLM-L6-v2`.\n",
    "4.  **Store:** Save vectors in ChromaDB.\n",
    "5.  **Retrieve:** Fetch relevant context based on user queries.\n",
    "6.  **Generate:** Use Llama-3 via Groq to answer questions using the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d5d5c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "Create a `.env` file in your project directory containing your API keys:\n",
    "```text\n",
    "GROQ_API_KEY=your_groq_api_key_here\n",
    "HF_TOKEN=your_huggingface_token_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6751351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (Uncomment to run)\n",
    "# !pip install langchain langchain-community langchain-groq langchain-huggingface chromadb python-dotenv bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4c6ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized: llama-3.1-8b-instant\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain Imports\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# Load Environment Variables\n",
    "load_dotenv()  # This looks for a .env file in the current directory\n",
    "\n",
    "# Verify API Keys are loaded\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not groq_api_key or not hf_token:\n",
    "    raise ValueError(\"Please ensure GROQ_API_KEY and HF_TOKEN are set in your .env file.\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=groq_api_key, \n",
    "    model_name=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "print(f\"LLM Initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd442083",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion (Document Loading)\n",
    "We will load a comprehensive blog post on \"LLM Powered Autonomous Agents\" by Lilian Weng. We use `SoupStrainer` to parse only the relevant content (title, headers, and content), ignoring navigation bars and footers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f193e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Document Title: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "Content Snippet: \n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In...\n"
     ]
    }
   ],
   "source": [
    "# Load the document from the web\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Display a snippet of the loaded content\n",
    "print(f\"Loaded Document Title: {docs[0].metadata['source']}\")\n",
    "print(f\"Content Snippet: {docs[0].page_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24b67c",
   "metadata": {},
   "source": [
    "## 3. Splitting and Embedding\n",
    "LLMs have a context window limit. To handle large documents, we:\n",
    "1.  **Split** the text into smaller chunks (1000 characters) with overlap (200 characters) to preserve context between chunks.\n",
    "2.  **Embed** these chunks into vectors using a HuggingFace model.\n",
    "3.  **Store** them in ChromaDB for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23f104e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 63 chunks.\n"
     ]
    }
   ],
   "source": [
    "# 1. Text Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"Document split into {len(splits)} chunks.\")\n",
    "\n",
    "# 2. Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3. Vector Store\n",
    "vector_store = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae096bc1",
   "metadata": {},
   "source": [
    "## 4. RAG Pipeline V1: Single Question Answering\n",
    "We create a standard retrieval chain that:\n",
    "1. Takes a user question.\n",
    "2. Retrieves relevant document chunks.\n",
    "3. Inserts chunks into a system prompt.\n",
    "4. Generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c96a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Define the System Prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the Chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Test the Chain\n",
    "response = rag_chain.invoke({\"input\": \"What is the definition of an agent?\"})\n",
    "print(\"Answer:\", response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eac69c",
   "metadata": {},
   "source": [
    "## 5. RAG Pipeline V2: Conversational RAG (With Memory)\n",
    "To support a chat interface, we need the system to understand follow-up questions (e.g., \"Tell me more about *that*\").\n",
    "\n",
    "We use a **History Aware Retriever**. This step reformulates the user's latest question based on the chat history to make it a standalone query that can be used for searching the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161783d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Contextualize Question Prompt\n",
    "# This prompt helps the LLM understand references to previous messages\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create History Aware Retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# 2. QA Prompt (Answer Generation)\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. Final Conversational Chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "conversational_rag_chain = create_retrieval_chain(\n",
    "    history_aware_retriever, question_answer_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690e977",
   "metadata": {},
   "source": [
    "### Testing the Conversational Interface\n",
    "We will simulate a conversation where the second question relies on the context of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d5e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is Self-Reflection in agents?\n",
      "AI: In agents, self-reflection is a process that enables them to improve iteratively by refining past action decisions and correcting previous mistakes. It helps agents learn from their experiences and adapt to new situations.\n",
      "\n",
      "Human: How is it created?\n",
      "AI: Self-reflection in agents is created by showing two-shot examples to the Large Language Model (LLM), where each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan).\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chat History\n",
    "chat_history = []\n",
    "\n",
    "# Question 1\n",
    "question1 = \"What is Self-Reflection in agents?\"\n",
    "response1 = conversational_rag_chain.invoke(\n",
    "    {\"input\": question1, \"chat_history\": chat_history}\n",
    ")\n",
    "\n",
    "print(f\"Human: {question1}\")\n",
    "print(f\"AI: {response1['answer']}\\n\")\n",
    "\n",
    "# Update History\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question1),\n",
    "        AIMessage(content=response1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Question 2 (Follow-up)\n",
    "question2 = \"How is it created?\" \n",
    "# The model must understand 'it' refers to 'Self-Reflection'\n",
    "response2 = conversational_rag_chain.invoke(\n",
    "    {\"input\": question2, \"chat_history\": chat_history}\n",
    ")\n",
    "\n",
    "print(f\"Human: {question2}\")\n",
    "print(f\"AI: {response2['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
